---
layout: post
title: Problem Sheet 6
subtitle: Zachary Lim
---

1. I imported the dataset and then explored it by using `glimpse`, `head`, and `summary`. 

    ```R    
    # load the csv
    smoking_df <- read_csv("smoking.csv")
    
    # explore the dataset
    glimpse(smoking_df)
    ## Rows: 1,000
    ## Columns: 3
    ## $ years  <dbl> 22.06873, 28.72932, 22.55478, 26.07060, 24.67653, 24.89933, ...
    ## $ smoker <dbl> 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, ...
    ## $ age    <dbl> 46.46951, 37.51216, 52.60310, 52.65284, 50.11282, 46.79830, ...
    
    head(smoking_df)
    ## # A tibble: 6 x 3
    ##   years smoker   age
    ##   <dbl>  <dbl> <dbl>
    ## 1  22.1      0  46.5
    ## 2  28.7      0  37.5
    ## 3  22.6      1  52.6
    ## 4  26.1      0  52.7
    ## 5  24.7      0  50.1
    ## 6  24.9      1  46.8
    
    summary(smoking_df)
    ##      years           smoker           age       
    ##  Min.   :11.83   Min.   :0.000   Min.   :21.28  
    ##  1st Qu.:20.51   1st Qu.:0.000   1st Qu.:50.25  
    ##  Median :22.69   Median :0.000   Median :57.61  
    ##  Mean   :22.64   Mean   :0.191   Mean   :56.85  
    ##  3rd Qu.:24.90   3rd Qu.:0.000   3rd Qu.:63.89  
    ##  Max.   :31.94   Max.   :1.000   Max.   :89.47
    ```

2. To plot the data, I needed to split the data into quartiles. I chose two ways of splitting them - the first to split them into equal groups and the second to split them into equal age ranges. Both showed generally that smokers had lower number of "years left to live" and that there was a generally negative correlation between age and years to live. However, both faced the issue of there being not many smokers into certain quartiles.

    ```R
    # splitting into equal numbers
    smoking_df <- smoking_df %>% mutate(age_round = (round(age,0))) %>%
      mutate(quartile = cut_number(age_round,4))

    smoking_df$quartile <- factor(smoking_df$quartile, labels = c("21 - 50", "51 - 58", "59 - 64", "65 - 89"))

	plot <- smoking_df %>% ggplot(aes(x=age, y=years,color=as.factor(smoker))) +
	  geom_point(alpha=0.3) +
	  geom_smooth(method="lm", se=F) +
	  labs(title = "Association between smoking and years to live (eq numbers)") + xlab("Age") + ylab("Years to live") +
	  facet_wrap(~quartile,scales = "free") +
	  scale_color_brewer(name  ="Smoker?",
			    labels=c("No", "Yes"),
			     palette = "Dark2") +
  	theme_bw()

	#plot the graph
	plot
	
    # splitting into equal age ranges
    smoking_df_2 <- smoking_df %>% mutate(age_round = (round(age,0))) %>%
      mutate(quartile = cut_interval(age_round,4))

    smoking_df_2$quartile <- factor(smoking_df_2$quartile, labels = c("21 - 38", "39 - 55", "56 - 72", "73 - 89"))

    plot2 <- smoking_df_2 %>% ggplot(aes(x=age, y=years,color=as.factor(smoker))) +
      geom_point(alpha=0.3) +
      geom_smooth(method="lm", se=F) +
      labs(title = "Association between smoking and years to live (eq range)") + xlab("Age") + ylab("Years to live") +
      facet_wrap(~quartile,scales = "free") +
      scale_color_brewer(name  ="Smoker?",
                         labels=c("No", "Yes"),
                         palette = "Dark2") +
      theme_bw()
    
    #plot the graph
    plot2
    ```

![image](https://user-images.githubusercontent.com/68678549/100498641-976c8f00-319e-11eb-92d5-199d23821567.png)

![image](https://user-images.githubusercontent.com/68678549/100498657-b1a66d00-319e-11eb-8c1a-0d3f32916000.png)

3. I regressed years on smoker and it was interesting to see that there appeared to be a positive effect on smoking and number of years left to live (approximately 2.896 years added for being a smoker). 

    ```R
    #create the lm model
    lm(smoking_df$years ~ smoking_df$smoker)
    ## 
    ## Call:
    ## lm(formula = smoking_df$years ~ smoking_df$smoker)
    ## 
    ## Coefficients:
    ##       (Intercept)  smoking_df$smoker  
    ##            22.090              2.896
    ```

4. However, we then controlled for age and this resulted in a shift in the results. Here, we see that being a smoker has a negative ~1.75 years effect on years to live. Age, likewise, has a similarly negative effect on number of years to live. The suggests that our earlier finding was flawed. Without controlling for age, it appears that we reached a wrong conclusion on the relationship between smoking and years left to live. 

    ```R
    #create lm model, controlling for age
    lm(smoking_df$years ~ smoking_df$smoker + smoking_df$age)
    ## 
    ## Call:
    ## lm(formula = smoking_df$years ~ smoking_df$smoker + smoking_df$age)
    ## 
    ## Coefficients:
    ##       (Intercept)  smoking_df$smoker     smoking_df$age  
    ##           39.6069            -1.7517            -0.2925
    ```

5. I ran `summary()` on the model. 

    ```R
    #saving model for summary
    model <-lm(smoking_df$years ~ smoking_df$smoker + smoking_df$age)

    #view summary of model
    summary(model)
    ## 
    ## Call:
    ## lm(formula = smoking_df$years ~ smoking_df$smoker + smoking_df$age)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -6.1704 -1.3111  0.0578  1.3495  6.7682 
    ## 
    ## Coefficients:
    ##                    Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)       39.606862   0.458148  86.450   <2e-16 ***
    ## smoking_df$smoker -1.751731   0.199636  -8.775   <2e-16 ***
    ## smoking_df$age    -0.292493   0.007561 -38.684   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.982 on 997 degrees of freedom
    ## Multiple R-squared:  0.6469, Adjusted R-squared:  0.6462 
    ## F-statistic: 913.3 on 2 and 997 DF,  p-value: < 2.2e-16
    ```

    Here, we see that both smoking and age have a statistically significant relationship with number of years left to live. This can be seen by the "***", showing a p-value of <0.001. Summary hence gives us an overview of the model as well as a number of parameters that allow us to ascertain the accuracy/fit of the model. 

6. We then constructed the 95% confidence interval for each parameter estimates as follows:

    ```R
    #construct 95% confidence interval 
    std.err <- sqrt(diag(vcov(model)))
    coeff <- model$coefficients
    
    #print out confidence intervals 
    for (i in 1:3){
      cat(sprintf("95%% Confidence Interval for %s: %f \u00b1 %f \n", names(coeff[i]), coeff[i], (2*std.err[i])))
    }
    ## 95% Confidence Interval for (Intercept): 39.606862 ± 0.916296 
    ## 95% Confidence Interval for smoking_df$smoker: -1.751731 ± 0.399273 
    ## 95% Confidence Interval for smoking_df$age: -0.292493 ± 0.015122
    ```

    The Central Limit Theorem states that if one were to take sufficiently large random samples from a population, then the mean of those samples would resemble a normal distribution. The 68-95-99 rule follows that 68% of observations within a normal distribution would lie within one standard deviation of the mean, 95% lie between two, and       99% lie within three standard deviations. In this case, given the relatively large sample size and as per the Central Limit Theorem, we can assume that (βˆ) would follow a normal distribution. Hence, if (βˆ) were to follow a normal distribution, then two standard deviations would allow for 95% of the observations to be contained, within which we would find the true β. 

    ```R
    #99% confidence intervals
    for (i in 1:3){
      cat(sprintf("99%% Confidence Interval for %s: %f \u00b1 %f \n", names(coeff[i]), coeff[i], (3*std.err[i])))
    }
    ## 99% Confidence Interval for (Intercept): 39.606862 ± 1.374444 
    ## 99% Confidence Interval for smoking_df$smoker: -1.751731 ± 0.598909 
    ## 99% Confidence Interval for smoking_df$age: -0.292493 ± 0.022683
    ```

    If we were to construct a 99% confidence interval, we would simply have to multiply SE(β) by three instead of two. 

7. As sample size increases, we see that the width of the confidence intervals decrease. This is because there is a decrease in the standard error since the standard error is calculated by σ/√n (population standard deviation over square root of sample size). Hence, as sample size increases, the standard error decreases. 

    ```R
    #creating sampling list
    sampling = list(20,50,100,500)
    
    #print confint for all sample sizes
    for (i in sampling){
      new_sample <- sample_n(smoking_df,i)
      model <- lm(new_sample$smoker ~ new_sample$years + new_sample$age)
      print(confint(model))
    }
    ##                        2.5 %       97.5 %
    ## (Intercept)       2.07724089  7.914885543
    ## new_sample$years -0.14937621 -0.001428515
    ## new_sample$age   -0.08288586 -0.026759197
    ##                        2.5 %       97.5 %
    ## (Intercept)       1.50735667  4.622805039
    ## new_sample$years -0.07236753  0.007524389
    ## new_sample$age   -0.04976550 -0.023325053
    ##                        2.5 %       97.5 %
    ## (Intercept)       1.82757522  4.025485061
    ## new_sample$years -0.05909308 -0.001599784
    ## new_sample$age   -0.04507090 -0.026866837
    ##                        2.5 %      97.5 %
    ## (Intercept)       2.24698875  3.22007234
    ## new_sample$years -0.04959490 -0.02425701
    ## new_sample$age   -0.03414887 -0.02613678
    ```

8. Looking at the model, this is how each section could be interpreted:

   `Call`: This is formula for the model that is being used 

   `Residual`: The residuals are the difference between the actual number of years left to live and those predicted by our model 

   `Coefficients`: These show the estimated values and standard error for each coefficient as well as their *t-values*. The standard error is an average of the difference between the predictor variable and the predicted variable should the model be run multiple times. The t-values provide a measure of how statistically significant the estimates are in relation to the predicted variable and can be calculated by multiplying the estimated value by the standard error. In this case, both smoking and the age of the person are statistically significant in determining the number of years left to live. This can be seen from the "significance codes" which show "***" for both variables of smoking and age. 

   `Residual Standard Error`: This is essentially the Root Mean Squared Error, which takes the square root of the sum of squared errors between the true and predicted values divided by *n* observations. However, in order to account for the number of variables used (and possible bias introduced), the RSE divides by the number of degrees of freedom (in this case 997 because 1000 observations and three parameters of intercept, smoking and age). 

   `R-Squared`: The R-squared value is a measure of how much variance is explained by the model. It can also be used as an estimate of the fit of the model. Adjusted R-Squared is R-Squared but adjusted for the number of parameters (since a greater number of parameters will typically necessitate a higher R-Squared). In this case, with an adjusted R-Squared of 0.6462, 64.4% of the variance of the number of years left to live can be explained by the model. 

   `F-Statistic`: The F-Statistic uses the F-test of overall significance. Essentially, it compares your model to a model that has no independent variables (i.e. just the intercept or mean). This tests if your model is better at modelling the predicted variable vis a vis using the mean. In this case, the p-value of the F-statistic is less than 0.001, allowing us to conclude that the model fits the data better than a model with no independent variables.   


